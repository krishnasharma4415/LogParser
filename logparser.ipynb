{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_df(df):\n",
    "    features = ['log']\n",
    "    target_labels = ['timestamp', 'LineId', 'Component', 'Content', 'EventId', 'EventTemplate']\n",
    "    label_encoders = {}\n",
    "    label_columns = []\n",
    "\n",
    "    df['log'] = df['log'].apply(preprocess_text)\n",
    "\n",
    "    for col in target_labels:\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "        label_columns.append(col + '_encoded')\n",
    "\n",
    "        logger.info(f\"Column: {col}\")\n",
    "        logger.info(f\"Unique values: {len(np.unique(df[col + '_encoded']))}\")\n",
    "        logger.info(f\"Min: {df[col + '_encoded'].min()}, Max: {df[col + '_encoded'].max()}\")\n",
    "        logger.info(\"---\")\n",
    "\n",
    "    return df, label_columns, label_encoders\n",
    "\n",
    "df, label_columns, label_encoders = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, logs, labels, tokenizer, max_len):\n",
    "        self.logs = logs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_labels = [len(np.unique(labels[:, i])) for i in range(labels.shape[1])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.logs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        log = str(self.logs[item])\n",
    "        labels = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            log,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'log_text': log,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRobertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(MultiTaskRobertaModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(self.roberta.config.hidden_size, num_label) for num_label in num_labels])\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.dropout(sequence_output[:, 0])\n",
    "\n",
    "        logits = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = [loss_fct(logit, label) for logit, label in zip(logits, labels.T)]\n",
    "            loss = sum(losses)\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "print(\"Unique label values:\", np.unique(df[label_columns].values))\n",
    "print(\"Max label value:\", np.max(df[label_columns].values))\n",
    "print(\"Min label value:\", np.min(df[label_columns].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "num_labels = [len(np.unique(df[col])) for col in label_columns]\n",
    "print(\"Number of labels for each task:\", num_labels)\n",
    "model = MultiTaskRobertaModel('roberta-base', num_labels)\n",
    "\n",
    "logs = df['log'].values.tolist()\n",
    "labels = df[label_columns].values\n",
    "\n",
    "dataset = LogDataset(logs, labels, tokenizer, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_warmup_steps = 0\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "    \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "    model = MultiTaskRobertaModel('roberta-base', num_labels).to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds = [[] for _ in range(len(label_columns))]\n",
    "        all_labels = [[] for _ in range(len(label_columns))]\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device),\n",
    "            }\n",
    "            labels = batch['labels'].to(device)\n",
    "            loss, logits = model(**inputs, labels=labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            for i, logit in enumerate(logits):\n",
    "                preds = logit.argmax(dim=-1).detach().cpu().numpy()\n",
    "                all_preds[i].extend(preds)\n",
    "                all_labels[i].extend(labels[:, i].cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = [[] for _ in range(len(label_columns))]\n",
    "        val_labels = [[] for _ in range(len(label_columns))]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {\n",
    "                    'input_ids': batch['input_ids'].to(device),\n",
    "                    'attention_mask': batch['attention_mask'].to(device),\n",
    "                }\n",
    "                labels = batch['labels'].to(device)\n",
    "                loss, logits = model(**inputs, labels=labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                for i, logit in enumerate(logits):\n",
    "                    preds = logit.argmax(dim=-1).detach().cpu().numpy()\n",
    "                    val_preds[i].extend(preds)\n",
    "                    val_labels[i].extend(labels[:, i].cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        logger.info(f\"Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}\")\n",
    "        logger.info(f\"Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        for i, (preds, labels) in enumerate(zip(val_preds, val_labels)):\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "            accuracy = accuracy_score(labels, preds)\n",
    "            \n",
    "            logger.info(f\"Task {i+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            \n",
    "            cm = confusion_matrix(labels, preds)\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(cm, annot=True, fmt='d')\n",
    "            plt.title(f'Confusion Matrix for Task {i+1}')\n",
    "            plt.ylabel('Actual')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.savefig(f'confusion_matrix_fold{fold+1}_task{i+1}_epoch{epoch+1}.png')\n",
    "            plt.close()\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, './best_model.pth')\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "\n",
    "print(f\"Best model had a validation loss of {best_val_loss}\")\n",
    "\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "\n",
    "tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_details(log_text, model, tokenizer, label_encoders, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(preprocess_text(log_text), return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)\n",
    "    \n",
    "    predictions = [logit.cpu().numpy().argmax(axis=1)[0] for logit in logits]\n",
    "    confidence_scores = [torch.softmax(logit, dim=1).max().item() for logit in logits]\n",
    "    \n",
    "    interpreted_predictions = {\n",
    "        'LineId': label_encoders['LineId'].inverse_transform([predictions[1]])[0],\n",
    "        'timestamp': label_encoders['timestamp'].inverse_transform([predictions[0]])[0],\n",
    "        'Component': label_encoders['Component'].inverse_transform([predictions[2]])[0],\n",
    "        'Content': label_encoders['Content'].inverse_transform([predictions[3]])[0],\n",
    "        'EventId': label_encoders['EventId'].inverse_transform([predictions[4]])[0],\n",
    "        'EventTemplate': label_encoders['EventTemplate'].inverse_transform([predictions[5]])[0]\n",
    "    }\n",
    "    \n",
    "    for key, value in interpreted_predictions.items():\n",
    "        interpreted_predictions[key] = {\n",
    "            'prediction': value,\n",
    "            'confidence': confidence_scores[list(interpreted_predictions.keys()).index(key)]\n",
    "        }\n",
    "\n",
    "    return interpreted_predictions\n",
    "\n",
    "model.load_state_dict(torch.load('./best_model.pth'))\n",
    "\n",
    "new_log = \"32,2016-09-28,04:30:31,Info,CBS,Warning: Unrecognized packageExtended attribute.,E50,Warning: Unrecognized packageExtended attribute.\"\n",
    "predicted_details = predict_log_details(new_log, model, tokenizer, label_encoders, device)\n",
    "print(predicted_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict_log_details(log_texts, model, tokenizer, label_encoders, device, batch_size=32):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(log_texts), batch_size):\n",
    "        batch_texts = log_texts[i:i+batch_size]\n",
    "        batch_texts = [preprocess_text(text) for text in batch_texts]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs)\n",
    "        \n",
    "        batch_predictions = [logit.cpu().numpy().argmax(axis=1) for logit in logits]\n",
    "        confidence_scores = [torch.softmax(logit, dim=1).max(dim=1)[0].cpu().numpy() for logit in logits]\n",
    "        \n",
    "        for j in range(len(batch_texts)):\n",
    "            predictions = [pred[j] for pred in batch_predictions]\n",
    "            scores = [score[j] for score in confidence_scores]\n",
    "            \n",
    "            interpreted_predictions = {\n",
    "                'LineId': label_encoders['LineId'].inverse_transform([predictions[1]])[0],\n",
    "                'timestamp': label_encoders['timestamp'].inverse_transform([predictions[0]])[0],\n",
    "                'Component': label_encoders['Component'].inverse_transform([predictions[2]])[0],\n",
    "                'Content': label_encoders['Content'].inverse_transform([predictions[3]])[0],\n",
    "                'EventId': label_encoders['EventId'].inverse_transform([predictions[4]])[0],\n",
    "                'EventTemplate': label_encoders['EventTemplate'].inverse_transform([predictions[5]])[0]\n",
    "            }\n",
    "            \n",
    "            for key, value in interpreted_predictions.items():\n",
    "                interpreted_predictions[key] = {\n",
    "                    'prediction': value,\n",
    "                    'confidence': scores[list(interpreted_predictions.keys()).index(key)]\n",
    "                }\n",
    "            \n",
    "            all_predictions.append(interpreted_predictions)\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "batch_logs = [new_log] * 5  # Just an example, replace with actual batch of logs\n",
    "batch_predictions = batch_predict_log_details(batch_logs, model, tokenizer, label_encoders, device)\n",
    "print(\"Batch predictions:\", batch_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
