{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "import logging\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm, trange\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22000 entries, 0 to 21999\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   timestamp      22000 non-null  object\n",
      " 1   LineId         22000 non-null  int64 \n",
      " 2   Component      22000 non-null  object\n",
      " 3   Content        22000 non-null  object\n",
      " 4   EventId        22000 non-null  object\n",
      " 5   EventTemplate  22000 non-null  object\n",
      " 6   log            22000 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df = df.head(22000)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Column: timestamp\n",
      "INFO:__main__:Unique values: 13167\n",
      "INFO:__main__:Min: 0, Max: 13166\n",
      "INFO:__main__:---\n",
      "INFO:__main__:Column: LineId\n",
      "INFO:__main__:Unique values: 2000\n",
      "INFO:__main__:Min: 0, Max: 1999\n",
      "INFO:__main__:---\n",
      "INFO:__main__:Column: Component\n",
      "INFO:__main__:Unique values: 299\n",
      "INFO:__main__:Min: 0, Max: 298\n",
      "INFO:__main__:---\n",
      "INFO:__main__:Column: Content\n",
      "INFO:__main__:Unique values: 11001\n",
      "INFO:__main__:Min: 0, Max: 11000\n",
      "INFO:__main__:---\n",
      "INFO:__main__:Column: EventId\n",
      "INFO:__main__:Unique values: 341\n",
      "INFO:__main__:Min: 0, Max: 340\n",
      "INFO:__main__:---\n",
      "INFO:__main__:Column: EventTemplate\n",
      "INFO:__main__:Unique values: 944\n",
      "INFO:__main__:Min: 0, Max: 943\n",
      "INFO:__main__:---\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def preprocess_df(df):\n",
    "    features = ['log']\n",
    "    target_labels = ['timestamp', 'LineId', 'Component', 'Content', 'EventId', 'EventTemplate']\n",
    "    label_encoders = {}\n",
    "    label_columns = []\n",
    "\n",
    "    df['log'] = df['log'].apply(preprocess_text)\n",
    "\n",
    "    for col in target_labels:\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "        label_columns.append(col + '_encoded')\n",
    "\n",
    "        logger.info(f\"Column: {col}\")\n",
    "        logger.info(f\"Unique values: {len(np.unique(df[col + '_encoded']))}\")\n",
    "        logger.info(f\"Min: {df[col + '_encoded'].min()}, Max: {df[col + '_encoded'].max()}\")\n",
    "        logger.info(\"---\")\n",
    "\n",
    "    return df, label_columns, label_encoders\n",
    "\n",
    "df, label_columns, label_encoders = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, logs, labels, tokenizer, max_len):\n",
    "        self.logs = logs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_labels = [len(np.unique(labels[:, i])) for i in range(labels.shape[1])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.logs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        log = str(self.logs[item])\n",
    "        labels = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            log,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'log_text': log,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values: [    0     1     2 ... 13164 13165 13166]\n",
      "Max label value: 13166\n",
      "Min label value: 0\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskRobertaModel(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super(MultiTaskRobertaModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(self.roberta.config.hidden_size, num_label) for num_label in num_labels])\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        pooled_output = self.dropout(sequence_output[:, 0])\n",
    "\n",
    "        logits = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = [loss_fct(logit, label) for logit, label in zip(logits, labels.T)]\n",
    "            loss = sum(losses)\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "print(\"Unique label values:\", np.unique(df[label_columns].values))\n",
    "print(\"Max label value:\", np.max(df[label_columns].values))\n",
    "print(\"Min label value:\", np.min(df[label_columns].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML DL\\LogParser\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels for each task: [13167, 2000, 299, 11001, 341, 944]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "num_labels = [len(np.unique(df[col])) for col in label_columns]\n",
    "print(\"Number of labels for each task:\", num_labels)\n",
    "model = MultiTaskRobertaModel('roberta-base', num_labels)\n",
    "\n",
    "logs = df['log'].values.tolist()\n",
    "labels = df[label_columns].values\n",
    "\n",
    "dataset = LogDataset(logs, labels, tokenizer, max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "num_warmup_steps = 0\n",
    "num_folds = 5\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = None\n",
    "# best_val_loss = float('inf')\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "#     print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "    \n",
    "#     train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "#     val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "    \n",
    "#     train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "#     val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "    \n",
    "#     model = MultiTaskRobertaModel('roberta-base', num_labels).to(device)\n",
    "    \n",
    "#     optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False)\n",
    "#     total_steps = len(train_loader) * num_epochs\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "#     for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         all_preds = [[] for _ in range(len(label_columns))]\n",
    "#         all_labels = [[] for _ in range(len(label_columns))]\n",
    "        \n",
    "#         progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "#         for batch in progress_bar:\n",
    "#             optimizer.zero_grad()\n",
    "#             inputs = {\n",
    "#                 'input_ids': batch['input_ids'].to(device),\n",
    "#                 'attention_mask': batch['attention_mask'].to(device),\n",
    "#             }\n",
    "#             labels = batch['labels'].to(device)\n",
    "#             loss, logits = model(**inputs, labels=labels)\n",
    "#             total_loss += loss.item()\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "            \n",
    "#             for i, logit in enumerate(logits):\n",
    "#                 preds = logit.argmax(dim=-1).detach().cpu().numpy()\n",
    "#                 all_preds[i].extend(preds)\n",
    "#                 all_labels[i].extend(labels[:, i].cpu().numpy())\n",
    "            \n",
    "#             progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "            \n",
    "#         avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_preds = [[] for _ in range(len(label_columns))]\n",
    "#         val_labels = [[] for _ in range(len(label_columns))]\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 inputs = {\n",
    "#                     'input_ids': batch['input_ids'].to(device),\n",
    "#                     'attention_mask': batch['attention_mask'].to(device),\n",
    "#                 }\n",
    "#                 labels = batch['labels'].to(device)\n",
    "#                 loss, logits = model(**inputs, labels=labels)\n",
    "#                 val_loss += loss.item()\n",
    "                \n",
    "#                 for i, logit in enumerate(logits):\n",
    "#                     preds = logit.argmax(dim=-1).detach().cpu().numpy()\n",
    "#                     val_preds[i].extend(preds)\n",
    "#                     val_labels[i].extend(labels[:, i].cpu().numpy())\n",
    "        \n",
    "#         avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "#         logger.info(f\"Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}\")\n",
    "#         logger.info(f\"Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "#         for i, (preds, labels) in enumerate(zip(val_preds, val_labels)):\n",
    "#             precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "#             accuracy = accuracy_score(labels, preds)\n",
    "            \n",
    "#             logger.info(f\"Task {i+1} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "            \n",
    "#             cm = confusion_matrix(labels, preds)\n",
    "#             plt.figure(figsize=(10, 8))\n",
    "#             sns.heatmap(cm, annot=True, fmt='d')\n",
    "#             plt.title(f'Confusion Matrix for Task {i+1}')\n",
    "#             plt.ylabel('Actual')\n",
    "#             plt.xlabel('Predicted')\n",
    "#             plt.savefig(f'confusion_matrix_fold{fold+1}_task{i+1}_epoch{epoch+1}.png')\n",
    "#             plt.close()\n",
    "\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             torch.save(best_model, './best_model.pth')\n",
    "#             print(f\"New best model saved with validation loss: {best_val_loss}\")\n",
    "\n",
    "\n",
    "# print(f\"Best model had a validation loss of {best_val_loss}\")\n",
    "\n",
    "# model.load_state_dict(torch.load('./best_model.pth'))\n",
    "\n",
    "# tokenizer.save_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15292\\3547099798.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_model.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = MultiTaskRobertaModel('roberta-base', [len(le.classes_) for le in label_encoders.values()])\n",
    "model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "model.eval()\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Log:\n",
      "  5,2016-09-28,04:30:31,Info,CBS,Ending TrustedInstaller initialization.,E17,Ending TrustedInstaller initialization.\n",
      "\n",
      "Predictions:\n",
      "  LineId:\n",
      "    Value: 1852\n",
      "    Confidence: 0.0313\n",
      "\n",
      "  timestamp:\n",
      "    Value: 2016-09-29 00:00:46\n",
      "    Confidence: 0.0021\n",
      "\n",
      "  Component:\n",
      "    Value: CBS\n",
      "    Confidence: 0.2770\n",
      "\n",
      "  Content:\n",
      "    Value: Interrupting SendWorker\n",
      "    Confidence: 0.0122\n",
      "\n",
      "  EventId:\n",
      "    Value: E7\n",
      "    Confidence: 0.1519\n",
      "\n",
      "  EventTemplate:\n",
      "    Value: Interrupting SendWorker\n",
      "    Confidence: 0.0196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def predict_log_details(log_text, model, tokenizer, label_encoders, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(preprocess_text(log_text), return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)\n",
    "    \n",
    "    predictions = [logit.cpu().numpy().argmax(axis=1)[0] for logit in logits]\n",
    "    confidence_scores = [torch.softmax(logit, dim=1).max().item() for logit in logits]\n",
    "    \n",
    "    interpreted_predictions = OrderedDict([\n",
    "        ('LineId', label_encoders['LineId'].inverse_transform([predictions[1]])[0]),\n",
    "        ('timestamp', label_encoders['timestamp'].inverse_transform([predictions[0]])[0]),\n",
    "        ('Component', label_encoders['Component'].inverse_transform([predictions[2]])[0]),\n",
    "        ('Content', label_encoders['Content'].inverse_transform([predictions[3]])[0]),\n",
    "        ('EventId', label_encoders['EventId'].inverse_transform([predictions[4]])[0]),\n",
    "        ('EventTemplate', label_encoders['EventTemplate'].inverse_transform([predictions[5]])[0])\n",
    "    ])\n",
    "    \n",
    "    result = OrderedDict()\n",
    "    result['input_log'] = log_text\n",
    "    result['predictions'] = OrderedDict()\n",
    "    \n",
    "    for i, (key, value) in enumerate(interpreted_predictions.items()):\n",
    "        result['predictions'][key] = {\n",
    "            'value': value,\n",
    "            'confidence': f\"{confidence_scores[i]:.4f}\"\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "new_log = \"5,2016-09-28,04:30:31,Info,CBS,Ending TrustedInstaller initialization.,E17,Ending TrustedInstaller initialization.\"\n",
    "predicted_details = predict_log_details(new_log, model, tokenizer, label_encoders, device)\n",
    "\n",
    "def print_predictions(predictions):\n",
    "    print(\"Input Log:\")\n",
    "    print(f\"  {predictions['input_log']}\")\n",
    "    print(\"\\nPredictions:\")\n",
    "    for key, value in predictions['predictions'].items():\n",
    "        print(f\"  {key}:\")\n",
    "        print(f\"    Value: {value['value']}\")\n",
    "        print(f\"    Confidence: {value['confidence']}\")\n",
    "        print()\n",
    "\n",
    "print_predictions(predicted_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict_log_details(log_texts, model, tokenizer, label_encoders, device, batch_size=32):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i in range(0, len(log_texts), batch_size):\n",
    "        batch_texts = log_texts[i:i+batch_size]\n",
    "        batch_texts = [preprocess_text(text) for text in batch_texts]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs)\n",
    "        \n",
    "        batch_predictions = [logit.cpu().numpy().argmax(axis=1) for logit in logits]\n",
    "        confidence_scores = [torch.softmax(logit, dim=1).max(dim=1)[0].cpu().numpy() for logit in logits]\n",
    "        \n",
    "        for j, original_text in enumerate(log_texts[i:i+batch_size]):\n",
    "            predictions = [pred[j] for pred in batch_predictions]\n",
    "            scores = [score[j] for score in confidence_scores]\n",
    "            \n",
    "            result = OrderedDict()\n",
    "            result['input_log'] = original_text\n",
    "            result['predictions'] = OrderedDict()\n",
    "            \n",
    "            for k, (key, le) in enumerate(label_encoders.items()):\n",
    "                result['predictions'][key] = {\n",
    "                    'value': le.inverse_transform([predictions[k]])[0],\n",
    "                    'confidence': f\"{scores[k]:.4f}\"\n",
    "                }\n",
    "            \n",
    "            all_predictions.append(result)\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:14: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_15292\\4015372975.py:14: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  \"2016-09-28 04:30:31, Info                  CBS    SQM: Failed to start upload with file pattern: C:\\Windows\\servicing\\sqm\\*_std.sqm, flags: 0x2 [HRESULT = 0x80004005 - E_FAIL]\",\n"
     ]
    }
   ],
   "source": [
    "batch_logs = [\"03-17 16:13:38.811  1702  2395 D WindowManager: printFreezingDisplayLogsopening app wtoken = AppWindowToken{9f4ef63 token=Token{a64f992 ActivityRecord{de9231d u0 com.tencent.qt.qtl/.activity.info.NewsDetailXmlActivity t761}}}, allDrawn= false, startingDisplayed =  false, startingMoved =  false, isRelaunching =  false\",\n",
    "              \"1117838976 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.36.156884 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\",\n",
    "              \"2015-10-18 18:01:50,572 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler\",\n",
    "              \"081109 204842 663 INFO dfs.DataNode$DataXceiver: Receiving block blk_1724757848743533110 src: /10.251.111.130:49851 dest: /10.251.111.130:50010\",\n",
    "              \"20171223-22:15:29:950|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7008##548365##8661##12456##27174269\",\n",
    "              \"360778 node-130 unix.hw state_change.unavailable 1141108031 1 Component State Change: Component \\042alt0\\042 is in the unavailable state (HWID=2478)\",\n",
    "              \"Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\",\n",
    "              \"Jul  1 09:04:37 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2\",\n",
    "              \"Dec 10 07:08:28 LabSZ sshd[24208]: reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!\",\n",
    "              \"nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:04.500 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Started (Lifecycle Event)\",\n",
    "              \"[10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1190 bytes (1.16 KB) sent, 1671 bytes (1.63 KB) received, lifetime 00:02\",\n",
    "              \"17/06/09 20:10:41 INFO storage.DiskBlockManager: Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0147/blockmgr-70293f72-844a-4b39-9ad6-fb0ad7e364e4\",\n",
    "              \"1131566461 2005.11.09 dn700 Nov 9 12:01:01 dn700/dn700 crond(pam_unix)[2912]: session opened for user root by (uid=0)\",\n",
    "              \"2016-09-28 04:30:31, Info                  CBS    SQM: Failed to start upload with file pattern: C:\\Windows\\servicing\\sqm\\*_std.sqm, flags: 0x2 [HRESULT = 0x80004005 - E_FAIL]\",\n",
    "              \"2015-07-29 19:13:27,721 - WARN  [RecvWorker:188978561024:QuorumCnxManager$RecvWorker@762] - Connection broken for id 188978561024, my id = 1, error = \",\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictions = batch_predict_log_details(batch_logs, model, tokenizer, label_encoders, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log 1:\n",
      "  Input: 03-17 16:13:38.811  1702  2395 D WindowManager: printFreezingDisplayLogsopening app wtoken = AppWindowToken{9f4ef63 token=Token{a64f992 ActivityRecord{de9231d u0 com.tencent.qt.qtl/.activity.info.NewsDetailXmlActivity t761}}}, allDrawn= false, startingDisplayed =  false, startingMoved =  false, isRelaunching =  false\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 2017-05-16 00:01:17.920\n",
      "      Confidence: 0.0005\n",
      "    LineId:\n",
      "      Value: 1745\n",
      "      Confidence: 0.0026\n",
      "    Component:\n",
      "      Value: symptomsd\n",
      "      Confidence: 0.0694\n",
      "    Content:\n",
      "      Value: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.\n",
      "      Confidence: 0.0008\n",
      "    EventId:\n",
      "      Value: E7\n",
      "      Confidence: 0.0432\n",
      "    EventTemplate:\n",
      "      Value: -[UABestAppSuggestionManager notifyBestAppChanged:type:options:bundleIdentifier:activityType:dynamicIdentifier:when:confidence:deviceName:deviceIdentifier:deviceType:] (null) UASuggestedActionType=<*> (null)/(null) opts=(null) when=<*>\n",
      "      Confidence: 0.0192\n",
      "\n",
      "Log 2:\n",
      "  Input: 1117838976 2005.06.03 R02-M1-N0-C:J12-U11 2005-06-03-15.49.36.156884 R02-M1-N0-C:J12-U11 RAS KERNEL INFO instruction cache parity error corrected\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 1131566461 2005.11.09 Nov 9 12:01:01\n",
      "      Confidence: 0.0129\n",
      "    LineId:\n",
      "      Value: 1909\n",
      "      Confidence: 0.0022\n",
      "    Component:\n",
      "      Value: switch_module\n",
      "      Confidence: 0.1463\n",
      "    Content:\n",
      "      Value: (root) CMD (run-parts /etc/cron.hourly)\n",
      "      Confidence: 0.0055\n",
      "    EventId:\n",
      "      Value: E13\n",
      "      Confidence: 0.1141\n",
      "    EventTemplate:\n",
      "      Value: reverse mapping checking getaddrinfo for <*> [<*>] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "      Confidence: 0.0312\n",
      "\n",
      "Log 3:\n",
      "  Input: 2015-10-18 18:01:50,572 INFO [main] org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventType for class org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: Jul 2 21:17:07\n",
      "      Confidence: 0.0005\n",
      "    LineId:\n",
      "      Value: 1233\n",
      "      Confidence: 0.0027\n",
      "    Component:\n",
      "      Value: 2 cport:-1)::PrepRequestProcessor\n",
      "      Confidence: 0.0310\n",
      "    Content:\n",
      "      Value: -[NetworkAnalyticsEngine _writeJournalRecord:fromCellFingerprint:key:atLOI:ofKind:lqm:isFaulty:] Hashing of the primary key failed. Dropping the journal record.\n",
      "      Confidence: 0.0008\n",
      "    EventId:\n",
      "      Value: E7\n",
      "      Confidence: 0.0390\n",
      "    EventTemplate:\n",
      "      Value: Established session <*> with negotiated timeout <*> for client /<*>:<*>\n",
      "      Confidence: 0.0191\n",
      "\n",
      "Log 4:\n",
      "  Input: 081109 204842 663 INFO dfs.DataNode$DataXceiver: Receiving block blk_1724757848743533110 src: /10.251.111.130:49851 dest: /10.251.111.130:50010\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 81110 105252\n",
      "      Confidence: 0.0010\n",
      "    LineId:\n",
      "      Value: 730\n",
      "      Confidence: 0.0026\n",
      "    Component:\n",
      "      Value: dfs.DataNode$DataXceiver\n",
      "      Confidence: 0.9978\n",
      "    Content:\n",
      "      Value: Receiving block blk_4742178675580222210 src: /10.251.42.9:60232 dest: /10.251.42.9:50010\n",
      "      Confidence: 0.0014\n",
      "    EventId:\n",
      "      Value: E13\n",
      "      Confidence: 0.9950\n",
      "    EventTemplate:\n",
      "      Value: Receiving block blk_<*> src: /<*>:<*> dest: /<*>:<*>\n",
      "      Confidence: 0.9980\n",
      "\n",
      "Log 5:\n",
      "  Input: 20171223-22:15:29:950|Step_SPUtils|30002312|setTodayTotalDetailSteps=1514038440000##7008##548365##8661##12456##27174269\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 20171223-23:17:40:996\n",
      "      Confidence: 0.0015\n",
      "    LineId:\n",
      "      Value: 948\n",
      "      Confidence: 0.0035\n",
      "    Component:\n",
      "      Value: Step_SPUtils\n",
      "      Confidence: 0.9978\n",
      "    Content:\n",
      "      Value: setTodayTotalDetailSteps=1514038680000##7139##548905##8661##16256##27396473\n",
      "      Confidence: 0.0012\n",
      "    EventId:\n",
      "      Value: E58\n",
      "      Confidence: 0.9936\n",
      "    EventTemplate:\n",
      "      Value: setTodayTotalDetailSteps=<*>\n",
      "      Confidence: 0.9967\n",
      "\n",
      "Log 6:\n",
      "  Input: 360778 node-130 unix.hw state_change.unavailable 1141108031 1 Component State Change: Component \"alt0\" is in the unavailable state (HWID=2478)\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 17/06/09 20:10:58\n",
      "      Confidence: 0.0010\n",
      "    LineId:\n",
      "      Value: 3\n",
      "      Confidence: 0.0025\n",
      "    Component:\n",
      "      Value: unix.hw\n",
      "      Confidence: 0.9323\n",
      "    Content:\n",
      "      Value: Cannot open channel to 2 at election address /10.10.34.12:3888\n",
      "      Confidence: 0.0031\n",
      "    EventId:\n",
      "      Value: E13\n",
      "      Confidence: 0.9038\n",
      "    EventTemplate:\n",
      "      Value: Component State Change: Component <*> is in the unavailable state (HWID=<*>)\n",
      "      Confidence: 0.6008\n",
      "\n",
      "Log 7:\n",
      "  Input: Jun 15 02:04:59 combo sshd(pam_unix)[20892]: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=220-135-151-1.hinet-ip.hinet.net  user=root\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: Jun 15 02:04:59\n",
      "      Confidence: 0.0181\n",
      "    LineId:\n",
      "      Value: 1181\n",
      "      Confidence: 0.0028\n",
      "    Component:\n",
      "      Value: sshd(pam_unix)\n",
      "      Confidence: 0.9972\n",
      "    Content:\n",
      "      Value: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=n219076184117.netvigator.com  user=root\n",
      "      Confidence: 0.1396\n",
      "    EventId:\n",
      "      Value: E18\n",
      "      Confidence: 0.9910\n",
      "    EventTemplate:\n",
      "      Value: authentication failure; logname= uid=0 euid=0 tty=NODEVssh ruser= rhost=<*>  user=root\n",
      "      Confidence: 0.9969\n",
      "\n",
      "Log 8:\n",
      "  Input: Jul  1 09:04:37 authorMacBook-Pro symptomsd[215]: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 2016-09-28 04:30:31\n",
      "      Confidence: 0.0027\n",
      "    LineId:\n",
      "      Value: 1442\n",
      "      Confidence: 0.0021\n",
      "    Component:\n",
      "      Value: symptomsd\n",
      "      Confidence: 0.7840\n",
      "    Content:\n",
      "      Value: __73-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value 2\n",
      "      Confidence: 0.6437\n",
      "    EventId:\n",
      "      Value: E50\n",
      "      Confidence: 0.9203\n",
      "    EventTemplate:\n",
      "      Value: __<*>-[NetworkAnalyticsEngine observeValueForKeyPath:ofObject:change:context:]_block_invoke unexpected switch value <*>\n",
      "      Confidence: 0.7227\n",
      "\n",
      "Log 9:\n",
      "  Input: Dec 10 07:08:28 LabSZ sshd[24208]: reverse mapping checking getaddrinfo for ns.marryaldkfaczcz.com [173.234.31.186] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: Dec 10 09:18:40\n",
      "      Confidence: 0.0040\n",
      "    LineId:\n",
      "      Value: 704\n",
      "      Confidence: 0.0027\n",
      "    Component:\n",
      "      Value: LabSZ\n",
      "      Confidence: 0.9989\n",
      "    Content:\n",
      "      Value: reverse mapping checking getaddrinfo for customer-187-141-143-180-sta.uninet-ide.com.mx [187.141.143.180] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "      Confidence: 0.8927\n",
      "    EventId:\n",
      "      Value: E27\n",
      "      Confidence: 0.9716\n",
      "    EventTemplate:\n",
      "      Value: reverse mapping checking getaddrinfo for <*> [<*>] failed - POSSIBLE BREAK-IN ATTEMPT!\n",
      "      Confidence: 0.9841\n",
      "\n",
      "Log 10:\n",
      "  Input: nova-compute.log.1.2017-05-16_13:55:31 2017-05-16 00:00:04.500 2931 INFO nova.compute.manager [req-3ea4052c-895d-4b64-9e2d-04d64c4d94ab - - - - -] [instance: b9000564-fe1a-409b-b8cc-1e88b294cd1d] VM Started (Lifecycle Event)\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 2017-05-16 00:02:55.757\n",
      "      Confidence: 0.0008\n",
      "    LineId:\n",
      "      Value: 1575\n",
      "      Confidence: 0.0025\n",
      "    Component:\n",
      "      Value: nova.compute.manager\n",
      "      Confidence: 0.9826\n",
      "    Content:\n",
      "      Value: [instance: 70c1714b-c11b-4c88-b300-239afe1f5ff8] VM Started (Lifecycle Event)\n",
      "      Confidence: 0.0014\n",
      "    EventId:\n",
      "      Value: E22\n",
      "      Confidence: 0.9385\n",
      "    EventTemplate:\n",
      "      Value: [instance: <*>] VM Started (Lifecycle Event)\n",
      "      Confidence: 0.9227\n",
      "\n",
      "Log 11:\n",
      "  Input: [10.30 16:49:08] chrome.exe - proxy.cse.cuhk.edu.hk:5070 close, 1190 bytes (1.16 KB) sent, 1671 bytes (1.63 KB) received, lifetime 00:02\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 17/06/09 20:10:48\n",
      "      Confidence: 0.0315\n",
      "    LineId:\n",
      "      Value: 1109\n",
      "      Confidence: 0.0027\n",
      "    Component:\n",
      "      Value: storage.MemoryStore\n",
      "      Confidence: 0.5835\n",
      "    Content:\n",
      "      Value: Authentication failed from 163.27.187.39 (163.27.187.39): Software caused connection abort\n",
      "      Confidence: 0.0022\n",
      "    EventId:\n",
      "      Value: E9\n",
      "      Confidence: 0.4581\n",
      "    EventTemplate:\n",
      "      Value: Block <*> stored as bytes in memory (estimated size <*>, free <*>)\n",
      "      Confidence: 0.3603\n",
      "\n",
      "Log 12:\n",
      "  Input: 17/06/09 20:10:41 INFO storage.DiskBlockManager: Created local directory at /opt/hdfs/nodemanager/usercache/curi/appcache/application_1485248649253_0147/blockmgr-70293f72-844a-4b39-9ad6-fb0ad7e364e4\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 2016-09-29 00:03:19\n",
      "      Confidence: 0.0007\n",
      "    LineId:\n",
      "      Value: 947\n",
      "      Confidence: 0.0029\n",
      "    Component:\n",
      "      Value: dfs.FSNamesystem\n",
      "      Confidence: 0.2973\n",
      "    Content:\n",
      "      Value: Interrupted while waiting for message on queue\n",
      "      Confidence: 0.0015\n",
      "    EventId:\n",
      "      Value: E7\n",
      "      Confidence: 0.7333\n",
      "    EventTemplate:\n",
      "      Value: BLOCK* NameSystem.allocateBlock: /<*>/part-<*>. blk_<*>\n",
      "      Confidence: 0.2610\n",
      "\n",
      "Log 13:\n",
      "  Input: 1131566461 2005.11.09 dn700 Nov 9 12:01:01 dn700/dn700 crond(pam_unix)[2912]: session opened for user root by (uid=0)\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 1131566461 2005.11.09 Nov 9 12:01:01\n",
      "      Confidence: 0.6191\n",
      "    LineId:\n",
      "      Value: 133\n",
      "      Confidence: 0.0035\n",
      "    Component:\n",
      "      Value: crond(pam_unix)\n",
      "      Confidence: 0.9560\n",
      "    Content:\n",
      "      Value: session opened for user root by (uid=0)\n",
      "      Confidence: 0.6719\n",
      "    EventId:\n",
      "      Value: E118\n",
      "      Confidence: 0.7675\n",
      "    EventTemplate:\n",
      "      Value: session opened for user root by (uid=0)\n",
      "      Confidence: 0.7828\n",
      "\n",
      "Log 14:\n",
      "  Input: 2016-09-28 04:30:31, Info                  CBS    SQM: Failed to start upload with file pattern: C:\\Windows\\servicing\\sqm\\*_std.sqm, flags: 0x2 [HRESULT = 0x80004005 - E_FAIL]\n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 2016-09-28 04:30:31\n",
      "      Confidence: 0.6354\n",
      "    LineId:\n",
      "      Value: 1727\n",
      "      Confidence: 0.0023\n",
      "    Component:\n",
      "      Value: CBS\n",
      "      Confidence: 0.9836\n",
      "    Content:\n",
      "      Value: Expecting attribute name [HRESULT = 0x800f080d - CBS_E_MANIFEST_INVALID_ITEM]\n",
      "      Confidence: 0.0385\n",
      "    EventId:\n",
      "      Value: E39\n",
      "      Confidence: 0.0835\n",
      "    EventTemplate:\n",
      "      Value: Failed to get next element [HRESULT = <*> - CBS_E_MANIFEST_INVALID_ITEM]\n",
      "      Confidence: 0.0512\n",
      "\n",
      "Log 15:\n",
      "  Input: 2015-07-29 19:13:27,721 - WARN  [RecvWorker:188978561024:QuorumCnxManager$RecvWorker@762] - Connection broken for id 188978561024, my id = 1, error = \n",
      "  Predictions:\n",
      "    timestamp:\n",
      "      Value: 17/06/09 20:10:53\n",
      "      Confidence: 0.0021\n",
      "    LineId:\n",
      "      Value: 1890\n",
      "      Confidence: 0.0035\n",
      "    Component:\n",
      "      Value: 188978561024:QuorumCnxManager$RecvWorker\n",
      "      Confidence: 0.9777\n",
      "    Content:\n",
      "      Value: Connection broken for id 188978561024, my id = 1, error =\n",
      "      Confidence: 0.9718\n",
      "    EventId:\n",
      "      Value: E11\n",
      "      Confidence: 0.9963\n",
      "    EventTemplate:\n",
      "      Value: Connection broken for id <*>, my id = <*>, error =\n",
      "      Confidence: 0.9955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_batch_predictions(predictions):\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Log {i + 1}:\")\n",
    "        print(f\"  Input: {prediction['input_log']}\")\n",
    "        print(\"  Predictions:\")\n",
    "        for key, value in prediction['predictions'].items():\n",
    "            print(f\"    {key}:\")\n",
    "            print(f\"      Value: {value['value']}\")\n",
    "            print(f\"      Confidence: {value['confidence']}\")\n",
    "        print()\n",
    "\n",
    "print_batch_predictions(batch_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
