{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bf373",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fixed_segments(template):\n",
    "    pattern = r\"(<\\*?>|<>|<<.*?>>)\"\n",
    "    segments = re.split(pattern, template)\n",
    "    fixed_parts = [seg.strip() for seg in segments if not re.match(pattern, seg) and seg.strip()]\n",
    "    return fixed_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bio_labels(content, template):\n",
    "    # Count number of expected variables\n",
    "    variable_count = template.count(\"<*>\") + template.count(\"<>\") + len(re.findall(r\"<<.*?>>\", template))\n",
    "\n",
    "    # Extract fixed segments\n",
    "    fixed_segments = extract_fixed_segments(template)\n",
    "    \n",
    "    # Escape fixed segments for regex pattern\n",
    "    escaped_segments = [re.escape(seg) for seg in fixed_segments]\n",
    "    \n",
    "    # Build pattern to extract variable parts between fixed segments\n",
    "    if escaped_segments:\n",
    "        split_pattern = \"(.*?)\".join(escaped_segments)\n",
    "        match = re.match(split_pattern, content)\n",
    "    else:\n",
    "        match = None\n",
    "\n",
    "    if match:\n",
    "        var_tokens = [v.strip() for v in match.groups()]\n",
    "    else:\n",
    "        print(f\"❗ Regex split failed: expected {variable_count} vars, but could not match.\")\n",
    "        print(f\"  Template: {template}\")\n",
    "        print(f\"  Content : {content}\")\n",
    "        return []\n",
    "\n",
    "    if len(var_tokens) != variable_count:\n",
    "        print(f\"⚠️ Mismatch: expected {variable_count} vars, found {len(var_tokens)}.\")\n",
    "        print(f\"  Template: {template}\")\n",
    "        print(f\"  Content : {content}\")\n",
    "        return []\n",
    "\n",
    "    # Tokenize content\n",
    "    content_tokens = tokenizer.tokenize(content)\n",
    "    labels = [\"O\"] * len(content_tokens)\n",
    "\n",
    "    # Match variable spans and assign BIO tags\n",
    "    for var in var_tokens:\n",
    "        var_toks = tokenizer.tokenize(var)\n",
    "        for i in range(len(content_tokens) - len(var_toks) + 1):\n",
    "            if content_tokens[i:i+len(var_toks)] == var_toks:\n",
    "                labels[i] = \"B-VAR\"\n",
    "                for j in range(1, len(var_toks)):\n",
    "                    labels[i+j] = \"I-VAR\"\n",
    "                break  # Stop after first match\n",
    "\n",
    "    return list(zip(content_tokens, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abda3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(csv_path, output_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        content = str(row[\"Content\"])\n",
    "        template = str(row[\"EventTemplate\"])\n",
    "        try:\n",
    "            tokens_and_labels = generate_bio_labels(content, template)\n",
    "            if tokens_and_labels:\n",
    "                examples.append(tokens_and_labels)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Skipping line due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in examples:\n",
    "            for token, label in sentence:\n",
    "                f.write(f\"{token} {label}\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6a715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_folder = \"../dataset/structured_data\"\n",
    "# output_folder = \"../dataset/bert_format\"\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# for file in os.listdir(input_folder):\n",
    "#     if file.endswith(\".csv\"):\n",
    "#         log_type = file.replace(\".csv\", \"\")\n",
    "#         print(f\"Processing {log_type}...\")\n",
    "#         process_file(\n",
    "#             csv_path=os.path.join(input_folder, file),\n",
    "#             output_path=os.path.join(output_folder, f\"{log_type}.txt\")\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bio_data(file_path):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_labels.append(labels)\n",
    "                    tokens = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    tokens.append(parts[0])\n",
    "                    labels.append(parts[1])\n",
    "    return {\"tokens\": all_tokens, \"labels\": all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b468d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"../dataset/bert_format\"\n",
    "\n",
    "dataset_list = []\n",
    "for file in os.listdir(folder):\n",
    "    if file.endswith(\".txt\") and \"mismatches\" not in file:\n",
    "        data = load_bio_data(os.path.join(folder, file))\n",
    "        dataset_list.append(Dataset.from_dict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23460f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = concatenate_datasets(dataset_list)\n",
    "dataset = full_dataset.train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e64658",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = [\"O\", \"B-VAR\", \"I-VAR\"]\n",
    "label2id = {l: i for i, l in enumerate(unique_labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "num_labels = len(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e07bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label = examples[\"labels\"][i]\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                # Same word, continue I- prefix\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727600a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc55514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=num_labels, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1327c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-log-parser\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=500,  \n",
    "    save_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee954d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_preds = [\n",
    "        [id2label[p] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(pred, lab) if l != -100]\n",
    "        for pred, lab in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    all_preds = sum(true_preds, [])\n",
    "    all_labels = sum(true_labels, [])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"micro\")\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34059a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-log-parser/final\")\n",
    "tokenizer.save_pretrained(\"./bert-log-parser/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b10ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
