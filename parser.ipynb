{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(structured,raw):\n",
    "    structured_df = pd.read_csv(structured)\n",
    "    with open(raw, 'r') as file:\n",
    "        data = file.read()\n",
    "        \n",
    "    lines = data.split('\\n')\n",
    "    cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "    raw_df = pd.DataFrame(cleaned_lines, columns=['log'])\n",
    "    \n",
    "    struct_df_reset = structured_df.reset_index(drop=True)\n",
    "    raw_df_reset = raw_df.reset_index(drop=True) \n",
    "    combined_df = pd.concat([raw_df_reset,struct_df_reset], axis=1)\n",
    "    df = combined_df.head(10)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "  # Convert Date and Time to timestamp\n",
    "  df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "  df['Timestamp'] = df['DateTime'].astype('int64') // 10**9\n",
    "  \n",
    "  # Encode categorical variables\n",
    "  label_encoders = {}\n",
    "  categorical_columns = ['Level', 'Component', 'EventId', 'EventTemplate']\n",
    "  for col in categorical_columns:\n",
    "      le = LabelEncoder()\n",
    "      df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "      label_encoders[col] = le\n",
    "  \n",
    "  # Select features for labels\n",
    "  label_columns = ['LineId', 'Timestamp', 'Level_encoded', 'Component_encoded', 'EventId_encoded', 'EventTemplate_encoded']\n",
    "  \n",
    "  return df, label_columns, label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "android_structured = \"structured_data\\Android_2k.log_structured.csv\"\n",
    "android_raw = \"raw_data\\Android_2k.log\"\n",
    "android_df = read(android_structured,android_raw)\n",
    "\n",
    "apache_structured = \"structured_data\\Apache_2k.log_structured.csv\"\n",
    "apache_raw = \"raw_data\\Apache_2k.log\"\n",
    "apache_df = read(apache_structured,apache_raw)\n",
    "\n",
    "bgl_structured = \"structured_data\\BGL_2k.log_structured.csv\"\n",
    "bgl_raw = \"raw_data\\BGL_2k.log\"\n",
    "bgl_df = read(bgl_structured,bgl_raw)\n",
    "\n",
    "handoop_structured = \"structured_data\\Hadoop_2k.log_structured.csv\"\n",
    "handoop_raw = \"raw_data\\Hadoop_2k.log\"\n",
    "handoop_df = read(handoop_structured,handoop_raw)\n",
    "\n",
    "hdfs_structured = \"structured_data\\HDFS_2k.log_structured.csv\"\n",
    "hdfs_raw = \"raw_data\\HDFS_2k.log\"\n",
    "hdfs_df = read(hdfs_structured,hdfs_raw)\n",
    "\n",
    "healthapp_structured = \"structured_data\\HealthApp_2k.log_structured.csv\"\n",
    "healthapp_raw = \"raw_data\\HealthApp_2k.log\"\n",
    "healthapp_df = read(healthapp_structured,healthapp_raw)\n",
    "\n",
    "hpc_structured = \"structured_data\\HPC_2k.log_structured.csv\"\n",
    "hpc_raw = \"raw_data\\HPC_2k.log\"\n",
    "hpc_df = read(hpc_structured,hpc_raw)\n",
    "\n",
    "linux_structured = \"structured_data\\Linux_2k.log_structured.csv\"\n",
    "linux_raw = \"raw_data\\Linux_2k.log\"\n",
    "linux_df = read(linux_structured,linux_raw)\n",
    "\n",
    "mac_structured = \"structured_data\\Mac_2k.log_structured.csv\"\n",
    "mac_raw = \"raw_data\\Mac_2k.log\"\n",
    "mac_df = read(mac_structured,mac_raw)\n",
    "\n",
    "openssh_structured = \"structured_data\\OpenSSH_2k.log_structured.csv\"\n",
    "openssh_raw = \"raw_data\\OpenSSH_2k.log\"\n",
    "openssh_df = read(openssh_structured,openssh_raw)\n",
    "\n",
    "openstack_structured = \"structured_data\\OpenStack_2k.log_structured.csv\"\n",
    "openstack_raw = \"raw_data\\OpenStack_2k.log\"\n",
    "openstack_df = read(openstack_structured,openstack_raw)\n",
    "\n",
    "proxifier_structured = \"structured_data\\Proxifier_2k.log_structured.csv\"\n",
    "proxifier_raw = \"raw_data\\Proxifier_2k.log\"\n",
    "proxifier_df = read(proxifier_structured,proxifier_raw)\n",
    "\n",
    "spark_structured = \"structured_data\\Spark_2k.log_structured.csv\"\n",
    "spark_raw = \"raw_data\\Spark_2k.log\"\n",
    "spark_df = read(spark_structured,spark_raw)\n",
    "\n",
    "thunderbird_structured = \"structured_data\\Thunderbird_2k.log_structured.csv\"\n",
    "thunderbird_raw = \"raw_data\\Thunderbird_2k.log\"\n",
    "thunderbird_df = read(thunderbird_structured,thunderbird_raw)\n",
    "\n",
    "windows_structured = \"structured_data\\Windows_2k.log_structured.csv\"\n",
    "windows_raw = \"raw_data\\Windows_2k.log\"\n",
    "windows_df = read(windows_structured,windows_raw)\n",
    "\n",
    "zookeeper_structured = \"structured_data\\Zookeeper_2k.log_structured.csv\"\n",
    "zookeeper_raw = \"raw_data\\Zookeeper_2k.log\"\n",
    "zookeeper_df = read(zookeeper_structured,zookeeper_raw)\n",
    "\n",
    "df = windows_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, label_columns, label_encoders = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogDataset(Dataset):\n",
    "    def __init__(self, logs, labels, tokenizer, max_len):\n",
    "        self.logs = logs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.logs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        log = str(self.logs[item])\n",
    "        labels = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            log,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'log_text': log,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.FloatTensor(labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = df['log'].tolist()\n",
    "labels = df[label_columns].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LogDataset(logs, labels, tokenizer, max_len=128)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ComputerScience\\AI ML DL\\LogParser\\.venv\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {'input_ids': batch['input_ids'], 'attention_mask': batch['attention_mask'], 'labels': batch['labels']}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./log_analysis_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_log_details(log_text):\n",
    "    inputs = tokenizer(log_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits.sigmoid()\n",
    "    raw_predictions = predictions.tolist()[0]\n",
    "\n",
    "    # Interpret predictions\n",
    "    interpreted_predictions = {}\n",
    "    \n",
    "    # LineId\n",
    "    interpreted_predictions['LineId'] = round(raw_predictions[0])\n",
    "    \n",
    "    # Timestamp to DateTime\n",
    "    timestamp = int(raw_predictions[1])\n",
    "    interpreted_predictions['DateTime'] = datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Level\n",
    "    level_encoded = round(raw_predictions[2])\n",
    "    interpreted_predictions['Level'] = label_encoders['Level'].inverse_transform([level_encoded])[0]\n",
    "    \n",
    "    # Component\n",
    "    component_encoded = round(raw_predictions[3])\n",
    "    interpreted_predictions['Component'] = label_encoders['Component'].inverse_transform([component_encoded])[0]\n",
    "    \n",
    "    # EventId\n",
    "    eventid_encoded = round(raw_predictions[4])\n",
    "    interpreted_predictions['EventId'] = label_encoders['EventId'].inverse_transform([eventid_encoded])[0]\n",
    "    \n",
    "    # EventTemplate\n",
    "    eventtemplate_encoded = round(raw_predictions[5])\n",
    "    interpreted_predictions['EventTemplate'] = label_encoders['EventTemplate'].inverse_transform([eventtemplate_encoded])[0]\n",
    "\n",
    "    return interpreted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LineId': 1, 'DateTime': '1970-01-01 05:30:00', 'Level': 'Info', 'Component': 'CSI', 'EventId': 'E17', 'EventTemplate': '<*>@<*>/<*>/<*>:<*>:<*>:<*>.<*> WcpInitialize (wcp.dll version <*>) called (stack @<*>)'}\n"
     ]
    }
   ],
   "source": [
    "new_log = \"32,2016-09-28,04:30:31,Info,CBS,Warning: Unrecognized packageExtended attribute.,E50,Warning: Unrecognized packageExtended attribute.\"\n",
    "predicted_details = predict_log_details(new_log)\n",
    "print(predicted_details)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
